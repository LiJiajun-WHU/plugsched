diff --git a/scheduler/kernel/sched/mod/fair.c b/scheduler/kernel/sched/mod/fair.c
--- a/scheduler/kernel/sched/mod/fair.c
+++ b/scheduler/kernel/sched/mod/fair.c
@@ -9500,6 +9500,15 @@ get_sd_balance_interval(struct sched_domain *sd, int cpu_busy)
 if (cpu_busy)
 interval *= sd->busy_factor;
 
+// Experimental: more aggressive load balancing
+if (sched_feat(CFS_LOAD_BALANCE_EXP)) {
+// Reduce balance interval by 20% for faster load distribution
+interval = interval * 8 / 10;
+if (interval < 1)
+interval = 1;
+}
+
 /* scale ms to jiffies */
 interval = msecs_to_jiffies(interval);
 interval = clamp(interval, 1UL, max_load_balance_interval);
diff --git a/scheduler/kernel/sched/mod/features.h b/scheduler/kernel/sched/mod/features.h
--- a/scheduler/kernel/sched/mod/features.h
+++ b/scheduler/kernel/sched/mod/features.h
@@ -6,6 +6,12 @@
  */
 SCHED_FEAT(CFS_VRUNTIME_EXP, false)
 
+/*
+ * CFS load balancing experiment
+ * More aggressive load balancing with reduced interval
+ */
+SCHED_FEAT(CFS_LOAD_BALANCE_EXP, false)
+
 /*
  * Only give sleepers 50% of their service deficit. This allows
